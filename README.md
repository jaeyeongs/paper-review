## [논문 리뷰]

#### BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
- https://arxiv.org/abs/1810.04805

#### 딥러닝 기반 한국어 띄어쓰기 모델: Korean Word-Spacing
- https://github.com/jaeyeongs/paper-review/blob/main/korean-spacing/KoreanWord-Spacing_20220427_%EC%8B%A0%EC%9E%AC%EC%98%81.pdf

#### Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks
- https://arxiv.org/abs/1908.10084
